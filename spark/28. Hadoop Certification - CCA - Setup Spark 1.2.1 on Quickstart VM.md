# 28. Hadoop Certification - CCA - Setup Spark 1.2.1 on Quickstart VM

* you need 1.2.1
	* alrady installed on exam vm
	* hadoop is 2.5
* installation
	* download 1.2.1
		* [wget](http://archive.apache.org/dist/spark/spark-1.2.1/spark-1.2.1-bin-hadoop2.4.tgz)
			* setup
				* we need to  copy /etc/spark/conf over to the 1.2.1
					* `cp  -rf /etc/spark/conf/* /home/cloudera/spark121/`
					* update the  conf to use 1.2.1
						* spark-defaults.conf
							* /home/cloudera/spark-1.2.1-bin-hadoop2.4/lib/spark-assembly-1.2.1-hadoop2.4.0.jar
						* classpath.txt
							* /home/cloudera/spark-1.2.1-bin-hadoop2.4/lib/spark-1.2.1-yarn-shuffle.jar
				* 1.2.1 needs to be configured too like the one in /usr/bin
				* in cmf
					* stop spark service
					* use  local mode
						* `bin/pyspark --master local`
						* --master yarn will not work because integration yarn 1.2.1
				* spark-env.sh
					* edit SPARK_HOME
				* change permission for job history
					* ```sudo -u hdfs hadoop fs -chmod 777 /user/spark
sudo -u spark hadoop fs -chmod 777 /user/spark/applicationHistory
```
			* precompiled  version for hadoop 2.4.x
* text
	* `bin/spark-shell -master local`
		* `sc.textFile("/user/hive/warehouse/sample_07").collect().foreach(println)`
		* `exit`
	* `bin/pyspark --master local`
		* ```
		sc.for recs in sc.textFile("/user/hive/warehouse/sample_07").collect():
            print(recs)
            ```
		* `exit()`
	* `bin/spark-sql --master local`
		* `select code, sum(t.salary) s  from (select code,  salary  from sample_07) t group by code order by s;`
* doc
	* [https://spark.apache.org/docs/1.2.1/](https://spark.apache.org/docs/1.2.1/)
* check current
	* `which spark-shell`
