# 33. Hadoop Certification - CCA - Pyspark - Reading and Saving Hive and JSON data

* obj
	* reading from hive tables
* HiveContext
	* >>> from pyspark.sql import HiveContext
>>> sqlContext = HiveContext(sc)
>>> data = sqlContext.sql("select * from sample_07")
		* for  i in data.collect():
  print(i)
		* access to specific table fields in the code
			* for  i in data.collect():
  print(i.code)
	* check hive conf in spark conf
	* json format
		* similar to xml but more humar readable
			* key and value pairs
			* {"department_id":2, "department_name":"Fitness"}
{"department_id":3, "department_name":"Footwear"}
{"department_id":4, "department_name":"Apparel"}
{"department_id":5, "department_name":"Golf"}
{"department_id":6, "department_name":"Outdoors"}
{"department_id":7, "department_name":"Fan Shop"}
{"department_id":8, "department_name":"TESTING"}
{"department_id":8000, "department_name":"TESTING"}
				* hadoop fs -put departments.json /user/cloudera/pyspark
		* read it
			* from pyspark import SQLContext
sqlContext = SQLContext(sc)
departmentsJson = sqlContext.jsonFile("/user/cloudera/pyspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
departmentsData = sqlContext.sql("select * from departmentsTable")
for rec in departmentsData.collect():
  print(rec)
				* jsonFile()
					* read a file from hdfs in json format
				* registerTempTable()
					* register a table from a RDD
						* you can write sql on it now
		* save it
			* #Writing data in json format
departmentsData.toJSON().saveAsTextFile("/user/cloudera/pyspark/departmentsJson")
				* toJSON()
					* you can print
					* you can save as file in hdfs
