# 29. Hadoop Certification - CCA - Introduction of pyspark (python on spark)

* section transform, stage, storage
* pyspark and sql
	* ```
	from pyspark.sql import HiveContext
    from pyspark.sql import SQLContext;
    sample = sqlContext.sql("select * from sample_07")
    for rec in sample.collect():
        print(rec)
  ```
		* use hive context
		* def queries
		* get rdd
		* apply action on the rdd
	* deoc
		* [http://spark.apache.org/docs/1.2.1/api/python/pyspark.sql.html](http://spark.apache.org/docs/1.2.1/api/python/pyspark.sql.html)
* execution
	* `pyspark --master local`
		* it does not use the cluster
	* pysaprk
		* native mode
		* check the yarn
* using  remote db
	* look for connector
	* ```
	os.environ['SPARK_CLASSPATH']="/usr/share/java/mysql-connector-java.jar"
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
jdbcurl="jdbc:mysql://quickstart.cloudera:3306/retail_db?user=root&password=cloudera"
df = sqlContext.load(source="jdbc", url=jdbcurl, dbtable="departments")
```
		* not evaluated yet
	* run the query
		* ```
		for rec in df.collect():
		    print(rec)
		    ```