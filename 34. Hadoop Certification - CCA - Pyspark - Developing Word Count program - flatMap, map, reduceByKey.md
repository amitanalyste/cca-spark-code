# 34. Hadoop Certification - CCA - Pyspark - Developing Word Count program - flatMap, map, reduceByKey

* rdd
	* [https://spark.apache.org/docs/1.2.1/programming-guide.html#resilient-distributed-datasets-rdds](https://spark.apache.org/docs/1.2.1/programming-guide.html#resilient-distributed-datasets-rdds)
	* parallelize()
		* transformations
			* map
			* filter
			* flatmap
			* sample
			* groupbykey
			* reducebykey
			* join
			* [https://spark.apache.org/docs/1.2.1/programming-guide.html#transformations](https://spark.apache.org/docs/1.2.1/programming-guide.html#transformations)
* lambda func
	* func to be called later
	* anonymous
	* def func(x): return x ** 3
	* single expression only
* load data
	* data = sc.textFile("/user/cloudera/wordcount.txt")
	* map
		* datamap = data.map(lambda x: (x.split(" "),1))
			* a line is mapped to 1
			* flatmap
				* >>> dataflatmap = data.flatMap(lambda x: x.split(" "))
					* now you have 1 word by row
					* >>> datamap = dataflatmap.map(lambda x: (x,1))
						* you get 
(u'Hello', 1)
(u'world..', 1)
(u'how', 1)
(u'are', 1)
							* apply the reduce func now on the key
								* >>> datared= datamap.reduceByKey(lambda x,y: x+y)
									* values of the same keys will be reduced 2 by 2 using the +
									* you get action so data is evaliuated
