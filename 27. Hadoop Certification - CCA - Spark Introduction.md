# 27. Hadoop Certification - CCA - Spark Introduction

* objective
	* convert data values
		* in hdfs
			* in another new one
		* using
			* python
			* scala
* spark version
	* certification
		* 1.2.1
* validate spark
	* spark-shell
		* scala cmd
			* ```
			val i: Int = 0
val str: String = new String("hello man")
```
		* use context
			* sqlcontext
				* more complicated to use
				* spark context itself with no hive involved
			* hivecontext
				* run sql queries using the spark instead on m/r
				* check of the conf files
					* /etc/hive/conf/hive-site.xml
					* /etc/hive/conf/hive-site.xml
					* create the sym link
						* `sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml`
						* you have hive support in spark
							* 17/02/24 03:34:49 INFO repl.SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContex
				* `sqlContext.sql("select * from test_07")`
					* get rdd with results
						* `.collect().foreach(println)`
						* `.count()`
					* install the samples from hue UI
					* ```
					scala> sqlContext.sql("select * from sample_07")
17/02/24 03:44:15 INFO parse.ParseDriver: Parsing command: select * from sample_07
17/02/24 03:44:15 INFO parse.ParseDriver: Parse Completed
res5: org.apache.spark.sql.DataFrame = [code: string, description: string, total_emp: int, salary: int]
```